#!/bin/sh
#
#    File: Scale_Test.ail
#
#   About: 
#
#   This file contains the SciDB AFL/AQL scripts we'll use to organize the 
#  scale and performance data. 
#
set -x
#
#  Phase 1. Load the "raw" data into a 1D Array. 
#
# Hygiene:
#
CMD="remove ( Raw_Timing_Data )"
iquery -aq "$CMD"
#
# Create the target array. 
#
CMD="
CREATE ARRAY Raw_Timing_Data <
  version : int64,
  scale_factor : int64,
  array_form : int64,
  query : int64,
  timing : double > 
[ Row=0:*,500,0]
"
iquery -aq "$CMD"
#
#  Load the data 
rm /tmp/load.pipe
mkfifo /tmp/load.pipe 
cat /tmp/scidb_load_format_timings_4_1000.csv | csv2scidb -c 500 -p NNNNN > /tmp/load.pipe & 
#
CMD="load(Raw_Timing_Data, '/tmp/load.pipe')"
iquery -aq "$CMD"
#
#  Phase 2: Organize the data into an array organized 
#       
#  Hygiene: 
#
CMD="remove ( Timing_Data )"
#
iquery -aq "$CMD"
#
# Create the 4D array. 
#
CMD="CREATE ARRAY Timing_Data <
  timing : double 
> 
[ version=0:*,1,0, scale_factor=0:64,100,0, 
  array_form=0:9,10,0, query=0:19,20,0 ]"
#
iquery -aq "$CMD"
#
#  Redim the data 
CMD="SELECT * INTO Timing_Data FROM Raw_Timing_Data"
iquery -nq "$CMD"
#
# Q1: How do we scale? 
#
#  For each version number, and each query, normalize the query's run-time, 
#  and calculate the average normalized run-time across all array types . . . 
#
CMD="
SELECT AVG(T1.timing / T2.timing) AS Avg_Normalized_Time
  FROM Timing_Data AS T1 JOIN slice ( Timing_Data, scale_factor, 1 ) AS T2 ON 
       T1.version = T2.version AND 
       T1.array_form = T2.array_form AND 
       T1.query = T2.query
  GROUP BY version, query;
"
iquery -o dcsv -q "$CMD"
#
#  Q2: How do we perform with sparsity? 
#
#  NOTE: This needs to change. The idea of the Array ID is that it is a value 
#        between 1 and 9. Arrays 1, 2, 3 are dense, 4, 5, 6 are 0.01, and 
#        7, 8, 9 are 0.0001 sparse. So - I will need to build a new array
#        with Sparseness as a dimension. 
#
#  Hygiene: 
#
CMD="remove ( Timing_by_Sparseness )"
iquery -aq "$CMD"
#
CMD="CREATE ARRAY Timing_by_Sparseness < 
  timing : double 
> [ version=0:*,1,0, scale_factor=0:64,100,0, 
    sparseness=0:3,4,0, query=0:*,20,0 ]"
#
iquery -aq "$CMD"
#
CMD="
SELECT AVG ( timing ) AS avg_timing
  FROM ( SELECT *, array_form / 3 as sparseness
           FROM Timing_Data )
REDIMENSION BY [ query=0:19,20,0, sparseness=0:4,5,0 ];
"
#
# Hygiene: 
# 
CMD="remove ( Times_by_Sparseness )"
iquery -aq "$CMD"
#
CMD="
store ( 
	redimension ( 
		apply ( Timing_Data,
           		sparseness,
           		( ( array_form - 1 ) / 3 ) + 1
   		),
   		< avg_timing : double null > 
                [ query=0:19,20,0, sparseness=0:4,5,0 ],
   		avg ( timing ) as avg_timing
	),
    Times_by_Sparseness
)
"
iquery -naq "$CMD"
#
CMD="
SELECT AVG(T1.avg_timing / T2.avg_timing) AS Avg_Normalized_Time
  FROM Times_by_Sparseness AS T1 JOIN 
       slice ( Times_by_Sparseness, sparseness, 1 ) AS T2 
       ON T1.query = T2.query
  GROUP BY sparseness;
"
iquery -o dcsv -q "$CMD"
#
#
# Q3: How does each query scale? 
#
#  The goal here is to come up with a graph that is: 
#   < query : integer,
#     scale_factor : integer, 
#     run_time : double > 
#  [ Cell=? ]
CMD="
SELECT AVG(T1.timing) AS avg_timing
 FROM Timing_Data  AS T1
REDIMENSION BY [ query=0:19,20,0, scale_factor=0:64,100,0 ]
"
iquery -q "${CMD}"

#
CMD="
redimension (
  Timing_Data,
  < timing : double, query : int64, scale_factor : int64 > 
  [ Num=0:*,100,0 ]
)
"
iquery -naq "$CMD"

